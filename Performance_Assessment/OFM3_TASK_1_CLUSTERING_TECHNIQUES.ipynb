{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OFM3 — OFM3 TASK 1: CLUSTERING TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Ryan L. Buchanan</li>\n",
    "<li>Student ID:  001826691</li>\n",
    "<li>Masters Data Analytics (12/01/2020)</li>\n",
    "<li>Program Mentor:  Dan Estes</li>\n",
    "<li>385-432-9281 (MST)</li>\n",
    "<li>rbuch49@wgu.edu</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 1\n",
    "One of the most critical factors in customer relationship management that directly affects a company’s long-term profitability is understanding its customers. When a company can better understand its customer characteristics, it is better able to target products and marketing campaigns for customers, resulting in better profits for the company in the long term.\n",
    "\n",
    "You are an analyst for a telecommunications company that wants to better understand the characteristics of its customers. You have been asked to perform a market basket analysis to analyze customer data to identify key associations of your customer purchases, ultimately allowing better business and strategic decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I: Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>A1. Proposal of Question</b>:</span>\n",
    "Which principal variables of your customers demonstrate that they are at high risk of churn?  And, therefore, which customers will churn?\n",
    "In other words, can we better understand our customers and identify patterns unique to customers who churn using unsupervised learning data mining?\n",
    "<br>This question will be answered using the <span style=\"color:red\"><b>K-means</b></span> clustering technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.  Define one goal of the data analysis. Ensure that your goal is reasonable within the scope of the scenario and is represented in the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>A2. Defined Goal</b>:</span>\n",
    "Stakeholders in the company will benefit by knowing, with some measure of confidence, which customers are at highest risk of churn because this will provide weight for decisions in marketing improved services to customers with these characteristics and past user experiences.\n",
    "The goal of this data analysis is to present numerical values to company stakeholders to help them better understand their customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II: Technique Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>B1. Explanation of Clustering Technique</b>:</span>\n",
    "<span style=\"color:red\">Explain how the clustering technique you chose analyzes the selected dataset.</span>\n",
    "    \n",
    "The two clustering techniques given on this task are hierarchical & k-means.  A DataCamp tutorial points out that \"\\[a\\] critical drawback of hierarchical clustering \\[is\\] runtime\" <span style=\"color:orange\">(Daityari, p. 1)</span>. While the dataset I am analyzing is not particularly large, the machine I am using is not particularly new, either. \n",
    "\n",
    "Also, I considered the dataset size and its patterns in order to decide on the k-means over hierarchical.  After all, customer churn is not separating countries soccer matches or about building a dendrogram.  What we need demonstrate to stakeholder is which groups (clusters) of customers are similar.  And, of course, how similar/different and tightly/loosely clustered are our groups of customers (market segments).\n",
    "Finally, this step is the exploratory phase of analysis.  Trial and error is acceptable at this point in the project.\n",
    "\n",
    "<span style=\"color:red\"><b><i>Include expected outcomes</i></b>.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>B2. Summary of Technique Assumption</b>:</span>\n",
    "\n",
    "Our key assumption in K-means clustering is, as VanderPlas points out, that the centroid or \"cluster center\" is the arithmetic mean of all points within or \"belonging to\" a cluster <span style=\"color:orange\">(VanderPlas, p. 463)</span>.\n",
    "\n",
    "Jeffares demonstrates that \"we wish to find the centroid <i>C</i> that minimises\" the distortion.\n",
    "    \n",
    "Where the distortion is measure of the variability of the observations within each cluster, also called the Within Cluster Sum of Squares (WCSS), <i>J</i>(<i>x</i>) is:\n",
    "\n",
    "$ J(x) = \\sum \\limits _{i=1} ^{m} || x_{i} - C || ^2 $\n",
    "\n",
    "\n",
    "Given the centroid <i>C</i>:  \n",
    "\n",
    "\n",
    "$ C = \\frac {\\sum \\limits _{i=1} ^{m} \\ x_{i}}{ m} $\n",
    "\n",
    "\n",
    "<span style=\"color:orange\">(Jeffares, p. 1).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>B3. Packages or Libraries List</b>:</span>\n",
    "<span style=\"color:red\">List the packages or libraries you have chosen for Python or R, and <span style=\"color:pink\">justify how <b><i>each</i></b> item</span> on the list supports the analysis.</span>\n",
    "\n",
    "The packages or libraries I have chosen for Python include:\n",
    "* Pandas\n",
    "* Numpy\n",
    "* Matplotlib\n",
    "* Seaborn\n",
    "* Scikit-learn\n",
    "* Scipy\n",
    "\n",
    "<br>Pandas, Numpy & Matplotlib are considered standard imports in a data science project, providing methods & statistical packages for reading, scoring & visualizing the data.  The Seaborn package provides more descriptive & visually intuitive graphs, matrices & plots.  The Scikit-learn packages efficiently implements methods for splitting, fitting, predicting & applying metrics for many machine learning models. Scipy offers a library for the kmeans algorithm and its methods as well as methods for scaling and normalization.\n",
    "\n",
    "<br>Also, IPython Jupyter notebooks will be used to support this analysis.  Python offers very intuitive, simple & versatile programming style & syntax, as well as a large system of mature packages for data science & machine learning.  Since, Python is cross-platform, it will work well whether consumers of the analysis are using Windows PCs or a MacBook laptop.  It is fast when compared with other possible programming languages like R or MATLAB <span style=\"color:orange\">(Massaron, p. 8)</span>.\n",
    "\n",
    "<br>Also, there is strong support for Python as the most popular data science programming language in popular literature & media <span style=\"color:orange\">(CBTNuggets, p. 1)</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>C1. Data Preprocessing</b>:</span>\n",
    "<span style=\"color:red\">Describe one data preprocessing goal relevant to the clustering technique from part A1.</span>\n",
    "\n",
    "A key preprocessing goal we need to consider is normalizing the data.  We will use continuous predictors for our clustering.  The range of values within the different predictors is quite wide.  Consider that age is on from . . . to . . . but income is from 00,000 to 000,000.  We have incomparable units.  And though we have comparable units, income and monthly fee for example, these same units have vastly different scales.  \n",
    "Using the data in its raw form, as such, may show unmeaningful bias in a clustering model.  Clearly, unscaled raw data clusters may be overstating the importance of one variable simply because of the magnitude of the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>C2. Dataset Variables</b>:</span>\n",
    "<span style=\"color:red\">Identify the initial dataset variables that you will use to perform the analysis for the clustering question from part A1, and label each as continuous or categorical.</span>\n",
    "\n",
    "Initial dataset variables used to perform the analysis for the classification analysis identified & classified as continuous or categorical below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>C3. Steps for Analysis</b>:</span>\n",
    "<span style=\"color:red\">Explain each of the steps used to prepare the data for the analysis. Identify the code segment for each step.</span>\n",
    "\n",
    "The steps used to prepare the dataset will include:\n",
    "<br>&ensp; 1. Back up my data and the process I am following as a copy to my machine and, since this is a manageable dataset, to GitHub using command line and gitbash.\n",
    "<br>&ensp; 2. Read the data set into Python using Pandas' read_csv command.\n",
    "<br>&ensp; 3. Evaluate the data struture to better understand input data using info & describe methods.\n",
    "<br>&ensp; 4. Naming the dataset as a the variable \"churn_df\" & subsequent useful slices of the dataframe as \"df\".\n",
    "<br>&ensp; 5. Examine potential misspellings, awkward variable naming & missing data.\n",
    "<br>&ensp; 6. Explore descriptive statistics for outliers that may create or hide statistical significance using histograms & box plots.\n",
    "<br>&ensp; 7. Where necessary, impute records missing data with meaningful measures of central tendency (mean, median or mode) or simply remove outliers that are several standard deviations above the mean.\n",
    "<br>&ensp; 8. Remove less meaningful categorical variables from dataset to provide fully numerical dataframe for further analysis.\n",
    "<br>&ensp; 9. Extract cleaned dataset as \"churn_df_prepared.csv\" for use in K-Nearest Neighbor model.\n",
    "\n",
    "Most relevant to our decision making process is the <b>dependent variable</b> of \"Churn\" which is binary categorical with only two values, \"Yes\" or \"No\".  \"Churn\" will be our <b>categorical target variable</b>. \n",
    "\n",
    "<br>In cleaning the data, we may discover relevance of the <b>continuous predictor variables</b>: \n",
    "* Children\n",
    "* Income\n",
    "* Outage_sec_perweek\n",
    "* Email\n",
    "* Contacts    \n",
    "* Yearly_equip_failure\n",
    "* Tenure (the number of months the customer has stayed with the provider)\n",
    "* MonthlyCharge\n",
    "* Bandwidth_GB_Year    \n",
    "    \n",
    "<br>Likewise, we may discover relevance of the <b>categorical predictor variables</b> (all binary categorical with only two values, \"Yes\" or \"No\", except where noted) The following will be encoded as dummy variables with 1/0: \n",
    "* Techie: Whether the customer considers themselves technically inclined (based on\n",
    "customer questionnaire when they signed up for services) (yes, no)\n",
    "* Contract: The contract term of the customer (month-to-month, one year, two year)\n",
    "* Port_modem: Whether the customer has a portable modem (yes, no)\n",
    "* Tablet: Whether the customer owns a tablet such as iPad, Surface, etc. (yes, no)\n",
    "* InternetService: Customer’s internet service provider (DSL, fiber optic, None)\n",
    "* Phone: Whether the customer has a phone service (yes, no)\n",
    "* Multiple: Whether the customer has multiple lines (yes, no)\n",
    "* OnlineSecurity: Whether the customer has an online security add-on (yes, no)\n",
    "* OnlineBackup: Whether the customer has an online backup add-on (yes, no)\n",
    "* DeviceProtection: Whether the customer has device protection add-on (yes, no)\n",
    "* TechSupport: Whether the customer has a technical support add-on (yes, no)\n",
    "* StreamingTV: Whether the customer has streaming TV (yes, no)\n",
    "* StreamingMovies: Whether the customer has streaming movies (yes, no)\n",
    "    \n",
    "<br>Finally, <b>discrete ordinal predictor variables</b> from the survey responses from customers regarding various customer service features may be relevant in the decision-making process. In the surveys, customers provided ordinal numerical data by rating 8 customer service factors on a scale of 1 to 8 (1 = most important, 8 = least important): \n",
    "    \n",
    "* Item1: Timely response\n",
    "* Item2: Timely fixes\n",
    "* Item3: Timely replacements\n",
    "* Item4: Reliability\n",
    "* Item5: Options\n",
    "* Item6: Respectful response\n",
    "* Item7: Courteous exchange\n",
    "* Item8: Evidence of active listening\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-learn\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Scipy\n",
    "from scipy.cluster.vq import kmeans, vq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change color of Matplotlib font\n",
    "import matplotlib as mpl\n",
    "\n",
    "COLOR = 'white'\n",
    "mpl.rcParams['text.color'] = COLOR\n",
    "mpl.rcParams['axes.labelcolor'] = COLOR\n",
    "mpl.rcParams['xtick.color'] = COLOR\n",
    "mpl.rcParams['ytick.color'] = COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Increase Jupyter display cell-width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore Warning Code\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set into Pandas dataframe\n",
    "churn_df = pd.read_csv('data/churn_clean.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine the features of the dataset\n",
    "churn_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea of dataset size\n",
    "churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View DataFrame info\n",
    "churn_df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data types of features\n",
    "churn_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine first few records of dataset\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of descriptive statistics\n",
    "churn_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove less relevant categorical variables from dataset\n",
    "churn_df = churn_df.drop(columns=['CaseOrder', 'Customer_id', 'Interaction', 'UID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename last 8 survey columns for better description of variables\n",
    "churn_df.rename(columns = {'Item1':'TimelyResponse', \n",
    "                    'Item2':'Fixes', \n",
    "                     'Item3':'Replacements', \n",
    "                     'Item4':'Reliability', \n",
    "                     'Item5':'Options', \n",
    "                     'Item6':'Respectfulness', \n",
    "                     'Item7':'Courteous', \n",
    "                     'Item8':'Listening'}, \n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create histograms of contiuous variables & categorical variables\n",
    "churn_df[['Children', 'Age', 'Income', 'Outage_sec_perweek', 'Email', \n",
    "          'Contacts', 'Yearly_equip_failure', 'Tenure', 'MonthlyCharge', \n",
    "          'Bandwidth_GB_Year', 'TimelyResponse', 'Courteous']].hist()\n",
    "plt.savefig('churn_pyplot.jpg')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot to get an idea of correlations between potentially related variables\n",
    "sns.scatterplot(x=churn_df['Outage_sec_perweek'], y=churn_df['Churn'], color='blue')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot to get an idea of correlations between potentially related variables\n",
    "sns.scatterplot(x=churn_df['Tenure'], y=churn_df['Churn'], color='blue')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot to get an idea of correlations between potentially related variables\n",
    "sns.scatterplot(x=churn_df['MonthlyCharge'], y=churn_df['Outage_sec_perweek'], color='blue')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a scatter matrix of numeric variables for high level overview of potential relationships & distributions\n",
    "churn_numeric = churn_df[['Children', 'Age', 'Income', 'Outage_sec_perweek', \n",
    "                          'Email', 'Contacts','Yearly_equip_failure', 'Tenure', \n",
    "                          'MonthlyCharge', 'Bandwidth_GB_Year', 'Replacements', \n",
    "                          'Reliability', 'Options', 'Respectfulness', 'Courteous', \n",
    "                          'Listening']]\n",
    "\n",
    "pd.plotting.scatter_matrix(churn_numeric, figsize = [15, 15]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual scatterplot for viewing relationship of key financial featurte against target variable\n",
    "sns.scatterplot(x = churn_df['MonthlyCharge'], y = churn_df['Churn'], color='red')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style to ggplot for aesthetics & R style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Countplot more useful than scatter_matrix when features of dataset are binary\n",
    "plt.figure()\n",
    "sns.countplot(x='Techie', hue='Churn', data=churn_df, palette='RdBu')\n",
    "plt.xticks([0,1], ['No', 'Yes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Countplot more useful than scatter_matrix when features of dataset are binary\n",
    "plt.figure()\n",
    "sns.countplot(x='PaperlessBilling', hue='Churn', data=churn_df, palette='RdBu')\n",
    "plt.xticks([0,1], ['No', 'Yes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot more useful than scatter_matrix when features of dataset are binary\n",
    "plt.figure()\n",
    "sns.countplot(x='InternetService', hue='Churn', data=churn_df, palette='RdBu')\n",
    "plt.xticks([0,1], ['No', 'Yes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple boxplots for continuous & categorical variables\n",
    "churn_df.boxplot(column=['MonthlyCharge','Bandwidth_GB_Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Seaborn boxplots for continuous & categorical variables\n",
    "sns.boxplot('MonthlyCharge', data = churn_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Seaborn boxplots for continuous & categorical variables\n",
    "sns.boxplot('Bandwidth_GB_Year', data = churn_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Seaborn boxplots for continuous variables\n",
    "sns.boxplot('Tenure', data = churn_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomalies\n",
    "It appears that anomolies have been removed from the supplied dataset, churn_clean.csv. &nbsp; There are no remaining outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover missing data points within dataset\n",
    "data_nulls = churn_df.isnull().sum()\n",
    "print(data_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data & visualize missing values in dataset \n",
    "\n",
    "# Install appropriate library\n",
    "!pip install missingno\n",
    "\n",
    "# Importing the libraries\n",
    "import missingno as msno\n",
    "\n",
    "# Visualize missing values as a matrix\n",
    "msno.matrix(churn_df);\n",
    "\"\"\"(GeeksForGeeks, p. 1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode binary categorical variables with dummies\n",
    "# churn_df['DummyGender'] = [1 if v == 'Male' else 0 for v in churn_df['Gender']]\n",
    "# churn_df['DummyChurn'] = [1 if v == 'Yes' else 0 for v in churn_df['Churn']] ### If the customer left (churned) they get a '1'\n",
    "# churn_df['DummyTechie'] = [1 if v == 'Yes' else 0 for v in churn_df['Techie']]\n",
    "# churn_df['DummyContract'] = [1 if v == 'Two Year' else 0 for v in churn_df['Contract']]\n",
    "# churn_df['DummyPort_modem'] = [1 if v == 'Yes' else 0 for v in churn_df['Port_modem']]\n",
    "# churn_df['DummyTablet'] = [1 if v == 'Yes' else 0 for v in churn_df['Tablet']]\n",
    "# churn_df['DummyInternetService'] = [1 if v == 'Fiber Optic' else 0 for v in churn_df['InternetService']]\n",
    "# churn_df['DummyPhone'] = [1 if v == 'Yes' else 0 for v in churn_df['Phone']]\n",
    "# churn_df['DummyMultiple'] = [1 if v == 'Yes' else 0 for v in churn_df['Multiple']]\n",
    "# churn_df['DummyOnlineSecurity'] = [1 if v == 'Yes' else 0 for v in churn_df['OnlineSecurity']]\n",
    "# churn_df['DummyOnlineBackup'] = [1 if v == 'Yes' else 0 for v in churn_df['OnlineBackup']]\n",
    "# churn_df['DummyDeviceProtection'] = [1 if v == 'Yes' else 0 for v in churn_df['DeviceProtection']]\n",
    "# churn_df['DummyTechSupport'] = [1 if v == 'Yes' else 0 for v in churn_df['TechSupport']]\n",
    "# churn_df['DummyStreamingTV'] = [1 if v == 'Yes' else 0 for v in churn_df['StreamingTV']]\n",
    "# churn_df['StreamingMovies'] = [1 if v == 'Yes' else 0 for v in churn_df['StreamingMovies']]\n",
    "# churn_df['DummyPaperlessBilling'] = [1 if v == 'Yes' else 0 for v in churn_df['PaperlessBilling']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original categorical features from dataframe\n",
    "# churn_df = churn_df.drop(columns=['Gender', 'Churn', 'Techie', 'Contract', 'Port_modem', 'Tablet', \n",
    "#                                   'InternetService', 'Phone', 'Multiple', 'OnlineSecurity', \n",
    "#                                   'OnlineBackup', 'DeviceProtection', 'TechSupport', \n",
    "#                                   'StreamingTV', 'StreamingMovies', 'PaperlessBilling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move DummyChurn to end of dataset to set as target\n",
    "# churn_df = churn_df[['Children', 'Age', 'Income', 'Outage_sec_perweek', 'Email', 'Contacts',\n",
    "#        'Yearly_equip_failure', 'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year',\n",
    "#         'TimelyResponse', 'Fixes', 'Replacements',\n",
    "#        'Reliability', 'Options', 'Respectfulness', 'Courteous', 'Listening',\n",
    "#        'DummyGender', 'DummyTechie', 'DummyContract',\n",
    "#        'DummyPort_modem', 'DummyTablet', 'DummyInternetService', 'DummyPhone',\n",
    "#        'DummyMultiple', 'DummyOnlineSecurity', 'DummyOnlineBackup',\n",
    "#        'DummyDeviceProtection', 'DummyTechSupport', 'DummyStreamingTV',\n",
    "#        'DummyPaperlessBilling', 'DummyChurn',]]\n",
    "\n",
    "# churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List features for analysis\n",
    "features = (list(churn_df.columns[:-1]))\n",
    "print('Features for analysis include: \\n', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>C4. Cleaned Dataset</b>:</span>\n",
    "<span style=\"color:red\">Provide a copy of the cleaned dataset.</span>\n",
    " \n",
    "\n",
    "Cleaned data set is extracted as \"churn_prepared.csv.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Clean dataset\n",
    "churn_df.to_csv('data/churn_prepared_kmeans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\"><b>Normalization of Dataset</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import normalization library from Scipy\n",
    "from scipy.cluster.vq import whiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = [5,1,3,3,2,3,3,8,1,2,2,3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data with the whiten method\n",
    "scaled_data = whiten(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\"><b>Plot a Visualization of the Normalization</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting library\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Initialize original, scaled data\n",
    "plt.plot(data, \n",
    "        label=\"original\")\n",
    "plt.plot(scaled_data,\n",
    "        label=\"scaled\")\n",
    "\n",
    "# Show legend and display plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Machine Learning A-Z</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prepared Churn dataset\n",
    "churn_df = pd.read_csv('data/churn_clean.csv', index_col=0)\n",
    "X = churn_df.iloc[:, :].values\n",
    "y = churn_df.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part IV: Analysis\n",
    "D. Perform the data analysis and report on the results by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-read fully numerical prepared dataset\n",
    "churn_df = pd.read_csv('data/churn_prepared_kmeans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>D1. Output and Intermediate Calculations</b></span>\n",
    "Describe the analysis technique you used to appropriately analyze the data. Include screenshots of the intermediate calculations you performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>D2. Code Execution</b></span>\n",
    "Provide the code used to perform the clustering analysis technique from part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cluster centers (code book) and labels\n",
    "cluster_centers, _ = kmeans(df[['scaled_x', 'scaled_y']], 3)\n",
    "df['cluster_labels'], _ = vq(df[['scaled_x', 'scaled_y']], cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters using Seaborn \n",
    "sns.scatterplot(x='scaled_x', y='scaled_y', hue='cluster_labels', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Another example of same'''\n",
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'], distortion_list = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Code to time something with'''\n",
    "import timeit\n",
    "\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Optimal clusters <i>K</i>: Elbow Method</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the variables for use\n",
    "distortions = []\n",
    "\n",
    "# Declare cluster number range\n",
    "num_clusters = range(2, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate distortions for various clusters\n",
    "for i in num_clusters:\n",
    "    centroids, distortion = kmeans(churn_df[['scaled_x', 'scaled_y']], i)\n",
    "    distortions.append(distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting elbow plot data\n",
    "elbow_plot_data = pd.DataFrame({'num_clusters': num_clusters,\n",
    "                               'distortions': distortions})\n",
    "\n",
    "sns.lineplot(x='num_clusters', y='distortions',\n",
    "            data=elbow_plot_data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"><b>Impact of seeds</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a random seed\n",
    "from numpy import random\n",
    "random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print initial accuracy score of KNN model\n",
    "print('Initial accuracy score KNN model: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute classification metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline object & scale dataframe\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set steps for pipeline object\n",
    "steps = [('scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier())]\n",
    "\n",
    "# Instantiate pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Split dataframe\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X, y, test_size = 0.2, random_state = SEED)\n",
    "\n",
    "# Scale dateframe with pipeline object\n",
    "knn_scaled = pipeline.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predict from scaled dataframe\n",
    "y_pred_scaled = pipeline.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print new accuracy score of scaled KNN model\n",
    "print('New accuracy score of scaled KNN model: {:0.3f}'.format(accuracy_score(y_test_scaled, y_pred_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute classification metrics after scaling\n",
    "print(classification_report(y_test_scaled, y_pred_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn confusion_matrix & generate results\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visually more intuitive confusion matrix\n",
    "\"\"\"(Dennis, pg. 1)\"\"\"\n",
    "group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part V: Data Summary and Implications\n",
    "E.  Summarize your data analysis by doing the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>E1. Accuracy of Clustering Technique</b></span>\n",
    "<span style=\"color:red\">Explain the accuracy of your clustering technique.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Comparison\n",
    "It appears that scaling improved model performance from an <b>Accuracy</b> of 0.71 to 0.79 & <b>Precision</b> of 0.78 to 0.84. The area under the curve is a decent score at 0.7959."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV for cross validation of model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up parameters grid\n",
    "param_grid = {'n_neighbors': np.arange(1, 50)}\n",
    "\n",
    "# Re-intantiate KNN for cross validation\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Instantiate GridSearch cross validation\n",
    "knn_cv = GridSearchCV(knn , param_grid, cv=5)\n",
    "\n",
    "# Fit model to \n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print('Best parameters for this KNN model: {}'.format(knn_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model best score\n",
    "print('Best score for this KNN model: {:.3f}'.format(knn_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import ROC AUC metrics for explaining the area under the curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Fit it to the data\n",
    "knn_cv.fit(X, y)\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = knn_cv.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute and print AUC score\n",
    "print(\"The Area under curve (AUC) on validation dataset is: {:.4f}\".format(roc_auc_score(y_test, y_pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-validated AUC scores: cv_auc\n",
    "cv_auc = cross_val_score(knn_cv, X, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Print list of AUC scores\n",
    "print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>E2. Results and Implications</b></span>\n",
    "<span style=\"color:red\">Discuss the results and implications of your clustering analysis.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>E3. Limitation</b></span>\n",
    " <span style=\"color:red\">Discuss <b><i>one</i></b> limitation of your data analysis. . . . <b><i>Random initialization trap?</i></b></span>\n",
    "\n",
    "\"When using the k-nearest neighbors algorithm you have the ability to change k, potentially yielding dramatically different results. You choose the value of <b><i>k</i></b> by trying different values and testing the prediction capabilities of the model. This means you must develop, validate, and test several models\" <span style=\"color:orange\">(Grant, pg. 1).</span>\n",
    "<br>What this means to our analysis here is that the relatively arbitrary choice of <b><i>k</i></b> = 7 nearest neighbors might yield dramatically different results if we chose a different <b><i>k</i></b> number of neighbors.  As discovered in our cross validation grid search, perhaps it should be the 6 nearest neighbors.\n",
    "Also, it appears to be memory intensive & computationally expensive.  Therefore, simply, it takes a long time to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>E4. Course of Action</b></span>\n",
    "<span style=\"color:red\">Recommend a course of action for the real-world organizational situation from part A1 based on your results and implications discussed in part E2.</span>\n",
    "    \n",
    "&emsp; It is critical that decision-makers & marketers understand that our predictor variables create a relatively low accuracy score with the results of an 0.84 after scaling. &nbsp; We should analyse the features that are in common among those leaving the company & attempt to reduce their likelihood of occuring with any given customer in the future. &nbsp; This suggests that as a customer subscribes to more services that the company provided, an additional port modem or online backup for example, they are less likely to leave the company. &nbsp; Clearly, it is the best interest of retaining customers to provide them with more services & improve their experience with the company by helping customers understand all the services that are available to them as a subscriber, not simple mobile phone service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"><b>F. Video</b></span>\n",
    "<span style=\"color:red\">link</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G. Sources for Third-Party Code\n",
    "* GeeksForGeeks. &ensp; (2019, July 4). &ensp; <i>Python | Visualize missing values (NaN) values using Missingno Library</i>. &ensp; GeeksForGeeks. &ensp; https://www.geeksforgeeks.org/python-visualize-missing-values-nan-values-using-missingno-library/\n",
    "<br>\n",
    "* SuperDataScience. &ensp; (2021, August 15) &ensp; <i>Machine Learning A-Z: Hands-On Python & R in Data Science</i>. &ensp; https://www.superdatascience.com/\n",
    "<br>\n",
    "<!-- * Dennis, T. &ensp; (2019, July 25). &ensp; <i>Confusion Matrix Visualization</i>. &ensp; Medium. &ensp; https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H. Sources \n",
    "* CBTNuggets. &ensp; (2018, September 20). &ensp; <i>Why Data Scientists Love Python</i>. &ensp; CBTNuggets. &ensp; https://www.cbtnuggets.com/blog/technology/data/why-data-scientists-love-python\n",
    "<br> \n",
    "* Daityari, S. &ensp; (2021, October 03). &ensp; <i>Basics of k-means clustering</i>. &ensp; DataCamp. &ensp; https://campus.datacamp.com/courses/cluster-analysis-in-python/k-means-clustering-3?ex=1\n",
    "<br> \n",
    "* Jeffares, A. &ensp; (2019, November 19). &ensp; <i>K-means: A Complete Introduction</i>. &ensp; TowardDataScience. &ensp; https://towardsdatascience.com/k-means-a-complete-introduction-1702af9cd8c\n",
    "<br> \n",
    "* Massaron, L. & Boschetti, A. &ensp; (2016). &ensp; <i>Regression Analysis with Python</i>. &ensp; Packt Publishing.\n",
    "<br> \n",
    "* VanderPlas, J. &ensp; (2017). &ensp; <i>Python Data Science Handbook</i>. &ensp; O'Reilly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py\n",
    "from colab_pdf import colab_pdf\n",
    "colab_pdf('D209 Data Mining 1 - NVM2 - Classification Analysis.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
